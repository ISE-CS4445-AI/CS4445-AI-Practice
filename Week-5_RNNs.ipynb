{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nyk7nQB_RKeL"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ISE-CS4445-AI/CS4445-AI-Practice/blob/main/Week-5_RNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "# Week 5 Exercise Notebook:\n",
        "## Practical RNN Implementation on IMDB Sentiment Analysis with PyTorch\n",
        "\n",
        "In this notebook, we will:\n",
        "- Load the IMDB dataset (movie reviews labeled as positive/negative).\n",
        "- Preprocess the text data (tokenization, vocabulary building, numericalization, and padding).\n",
        "- Build a simple RNN (using an LSTM cell) for sentiment classification.\n",
        "- Train the model on a small subset (for demonstration) and evaluate predictions.\n",
        "- Use the trained model to make a fun prediction on a custom review.\n",
        "\n",
        "> **Note:** Training on the full IMDB dataset will take time. For demonstration, we are going to use a subset of randomly sampled 1000 datapoints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsLjnO2TRKeV"
      },
      "source": [
        "---\n",
        "## Initial Setup & Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torchdata torchtext\n",
        "!pip install torchdata==0.6.1 torchtext==0.15.2"
      ],
      "metadata": {
        "id": "L_rom2TFRmIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install portalocker==2.7.0"
      ],
      "metadata": {
        "id": "M2ic1X5lWkhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-G6eTL-8RKeW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torchtext.datasets import IMDB\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter\n",
        "from torchtext.vocab import Vocab\n",
        "import random\n",
        "import portalocker\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPH5oP4BRKeY"
      },
      "source": [
        "## Loading and exploring the IMDB dataset\n",
        "\n",
        "But first, some background about text preprocessing.\n",
        "\n",
        "## Introduction to Text Preprocessing\n",
        "\n",
        "In this section, we prepare the raw text data for our RNN model. The IMDB dataset contains movie reviews along with their labels (\"pos\" or \"neg\"). Before feeding the data into our model, we need to perform several key steps:\n",
        "\n",
        "- **Tokenization:**  \n",
        "  Splitting the text into individual words or tokens. We use a basic English tokenizer from torchtext.\n",
        "\n",
        "- **Vocabulary Building:**  \n",
        "  Creating a mapping from words (tokens) to unique integer indices.  \n",
        "  This allows us to convert each review into a sequence of numbers.  \n",
        "  We also add special tokens like `<unk>` (for unknown words) and `<pad>` (for padding shorter sequences).\n",
        "\n",
        "- **Pipeline Creation:**  \n",
        "  Writing functions to transform raw text into numerical token IDs and mapping labels to integers.  \n",
        "  These pipelines standardize the data input before it reaches the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gImZ3l5wRKeY"
      },
      "outputs": [],
      "source": [
        "# Download the IMDB dataset using torchtext.\n",
        "# Each example is a tuple: (label, text)\n",
        "train_iter = list(IMDB(split='train'))\n",
        "test_iter = list(IMDB(split='test'))\n",
        "\n",
        "# For quick demonstration, we will use a small subset of the training data.\n",
        "# You can adjust the subset size if needed.\n",
        "subset_size = 1000\n",
        "train_data = random.sample(train_iter, subset_size)\n",
        "\n",
        "print(f\"Total training examples (subset): {len(train_data)}\")\n",
        "print(\"Example:\", train_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization and Building the Vocabulary\n",
        "\n",
        "We begin by defining a tokenizer using torchtextâ€™s `get_tokenizer` with the `basic_english` setting. This simple tokenizer splits the text on whitespace and punctuation, converting everything to lowercase.\n",
        "\n",
        "Next, we build the vocabulary. Rather than manually updating a Counter, we use an iterator approach:\n",
        "\n",
        "- **Yield Tokens:**  \n",
        "  A helper function (`yield_tokens`) goes through each review and yields the tokenized words.  \n",
        "  This is efficient for large datasets.\n",
        "\n",
        "- **Special Tokens:**  \n",
        "  We include `<unk>` for words that are not in our vocabulary and `<pad>` to pad sequences to the same length in a batch.\n",
        "\n",
        "- **Building the Vocab:**  \n",
        "  We use `build_vocab_from_iterator` to create the vocabulary from our tokens.  \n",
        "  We also set a default index using `vocab.set_default_index` so that any word not found in the vocab is mapped to `<unk>`.\n",
        "\n",
        "After the vocabulary is built, we define two pipelines:\n",
        "\n",
        "- **Text Pipeline:**  \n",
        "  Converts each review (a string) into a list of integers representing token IDs.\n",
        "\n",
        "- **Label Pipeline:**  \n",
        "  Maps the label \"pos\" to `1` and \"neg\" to `0`.\n",
        "\n",
        "These transformations allow our model to work with numerical data rather than raw text.\n"
      ],
      "metadata": {
        "id": "PC2UEdbDRxd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "# Define a tokenizer using torchtext's basic_english tokenizer.\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "# Build an iterator that yields tokens from the training data.\n",
        "def yield_tokens(data_iter):\n",
        "    for label, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "# Assume train_data is already defined (a list of (label, text) tuples).\n",
        "# Build the vocabulary from the training data iterator.\n",
        "specials = ['<unk>', '<pad>']\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_data), specials=specials)\n",
        "\n",
        "# Set the default index for unknown tokens.\n",
        "vocab.set_default_index(vocab['<unk>'])\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab)}\")\n",
        "\n",
        "# Define pipelines to convert text to token IDs and labels to integers.\n",
        "def text_pipeline(text):\n",
        "    return [vocab[token] for token in tokenizer(text)]\n",
        "\n",
        "def label_pipeline(label):\n",
        "    # Map \"pos\" to 1 and \"neg\" to 0.\n",
        "    return 1 if label == \"pos\" else 0\n"
      ],
      "metadata": {
        "id": "PoA0nUT9RYEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Variable-Length Sequences: Padding\n",
        "\n",
        "Movie reviews naturally have variable lengths. To process these reviews in batches, we need to ensure that all sequences in a batch have the same length.\n",
        "\n",
        "- **Padding:**  \n",
        "  We use `pad_sequence` from PyTorch to pad shorter sequences with a special `<pad>` token.  \n",
        "  This makes it possible to form a uniform tensor of shape `[batch_size, max_seq_length]`.\n",
        "\n",
        "- **Collate Function:**  \n",
        "  Our custom `collate_batch` function:\n",
        "  - Takes a batch of (text, label) pairs.\n",
        "  - Pads the text sequences to the length of the longest review in the batch.\n",
        "  - Returns a padded tensor of token IDs and a tensor of labels.\n",
        "\n",
        "This step is crucial for feeding batches into our RNN, which expects a consistent input size across the batch.\n",
        "\n",
        "## Create a custom dataset and dataloader"
      ],
      "metadata": {
        "id": "aVZrTD_2Xg-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        # Data is a list of tuples (label, text)\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label, text = self.data[idx]\n",
        "        return torch.tensor(text_pipeline(text)), torch.tensor(label_pipeline(label))\n",
        "\n",
        "# Custom collate function to pad sequences within a batch.\n",
        "def collate_batch(batch):\n",
        "    # Each item in batch is (text_tensor, label_tensor)\n",
        "    text_list, label_list = zip(*batch)\n",
        "    # Pad sequences to the maximum length in the batch.\n",
        "    text_list = pad_sequence(text_list, batch_first=True, padding_value=vocab['<pad>'])\n",
        "    labels = torch.tensor(label_list)\n",
        "    return text_list, labels\n",
        "\n",
        "# Create DataLoader objects.\n",
        "batch_size = 32\n",
        "train_dataset = IMDBDataset(train_data)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n"
      ],
      "metadata": {
        "id": "Tr5EeUl8Rs5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the RNN Model (LSTM-based Classifier)\n",
        "### RNNClassifier Architecture for Sentiment Analysis\n",
        "\n",
        "In this section, we define the `RNNClassifier` class. This custom model is designed to classify movie reviews (from the IMDB dataset) as positive or negative. The architecture uses an LSTM (Long Short-Term Memory) layer, which is a popular type of Recurrent Neural Network (RNN) that helps capture sequential dependencies and address issues like the vanishing gradient problem.\n",
        "\n",
        "### Key Components and Their Roles\n",
        "\n",
        "1. **Embedding Layer (`nn.Embedding`):**\n",
        "   - **Purpose:**  \n",
        "     Converts integer-encoded tokens (words) into dense, continuous vector representations.  \n",
        "   - **Details:**  \n",
        "     - The `vocab_size` parameter determines how many unique words the model can embed.  \n",
        "     - The `embed_dim` defines the size of each word vector.  \n",
        "     - The `padding_idx` is set to the index of the `<pad>` token, ensuring that padded elements do not contribute to the learning process.\n",
        "   - **Outcome:**  \n",
        "     The input text tensor (shape: `[batch_size, seq_length]`) becomes an embedded tensor of shape `[batch_size, seq_length, embed_dim]`.\n",
        "\n",
        "2. **LSTM Layer (`nn.LSTM`):**\n",
        "   - **Purpose:**  \n",
        "     Processes the embedded sequence data step-by-step, capturing the context and order within the review.\n",
        "   - **Details:**  \n",
        "     - Receives the embedded sequence as input.  \n",
        "     - The `hidden_dim` parameter controls the size of the hidden state vector.  \n",
        "     - The `num_layers` parameter specifies the depth (number of stacked LSTM layers).  \n",
        "     - `batch_first=True` indicates that the first dimension of the input represents the batch size.\n",
        "   - **Output:**  \n",
        "     - Returns two values:\n",
        "       1. The LSTM outputs at all time steps (not used in our model).\n",
        "       2. A tuple `(hidden, cell)` where:\n",
        "          - `hidden` contains the hidden state from each layer (shape: `[num_layers, batch_size, hidden_dim]`).\n",
        "          - We take `hidden[-1]` (i.e., the hidden state from the last LSTM layer) as a summary of the entire sequence.\n",
        "\n",
        "3. **Fully Connected Layer (`nn.Linear`):**\n",
        "   - **Purpose:**  \n",
        "     Maps the final hidden state from the LSTM to a single output value (a logit) that will be used for binary classification.\n",
        "   - **Details:**  \n",
        "     - It transforms a vector of size `hidden_dim` to `output_dim` (which is 1 for our binary sentiment classification).\n",
        "   - **Outcome:**  \n",
        "     Produces a logit for each example in the batch.\n",
        "\n",
        "4. **Sigmoid Activation (`nn.Sigmoid`):**\n",
        "   - **Purpose:**  \n",
        "     Converts the logit into a probability between 0 and 1.\n",
        "   - **Outcome:**  \n",
        "     After applying the sigmoid, values closer to 1 indicate a positive sentiment and values closer to 0 indicate a negative sentiment.\n",
        "   - **Note:**  \n",
        "     The `.squeeze()` function is applied to remove any extra dimensions, ensuring the output shape is appropriate for subsequent loss computation.\n",
        "\n",
        "### Walkthrough of the `forward` Method\n",
        "\n",
        "- **Input:**  \n",
        "  The `forward` method accepts `text`, a tensor of token IDs with shape `[batch_size, seq_length]`.\n",
        "\n",
        "- **Step 1: Embedding**  \n",
        "  ```python\n",
        "  embedded = self.embedding(text)  \n",
        "\n",
        "Each token is converted into a dense vector. The resulting tensor has shape `[batch_size, seq_length, embed_dim]`.\n",
        "\n",
        "\n",
        "- **Step 2: LSTM Processing**  \n",
        "  ```python  \n",
        "  output, (hidden, cell) = self.lstm(embedded)  \n",
        "\n",
        "The LSTM processes the embedded sequence. We are mainly interested in the final hidden state (`hidden`) which contains the learned representation for the entire sequence.\n",
        "\n",
        "- **Step 3: Extracting the Final Hidden State**\n",
        "```python\n",
        "hidden_last = hidden[-1]\n",
        "\n",
        "We select the hidden state from the last LSTM layer. This tensor has shape `[batch_size, hidden_dim]` and serves as the summary of the input review.\n",
        "\n",
        "- **Step 4: Fully Connected Mapping**\n",
        "```python\n",
        "logits = self.fc(hidden_last)\n",
        "\n",
        "The hidden summary is passed through the linear layer to generate a logit for each example.\n",
        "\n",
        "- **Step 5: Sigmoid Activation**\n",
        "```python\n",
        "return self.sigmoid(logits).squeeze()\n",
        "\n",
        "The logit is converted to a probability score between 0 and 1. The squeeze function ensures that the output has the correct dimensions."
      ],
      "metadata": {
        "id": "wgcUKozRXn2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, num_layers=1):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=vocab['<pad>'])\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()  # For binary classification\n",
        "\n",
        "    def forward(self, text):\n",
        "        # text shape: [batch_size, seq_length]\n",
        "        embedded = self.embedding(text)  # Shape: [batch_size, seq_length, embed_dim]\n",
        "        # LSTM returns (output, (hidden, cell)). We use the hidden state from the last time step.\n",
        "        output, (hidden, cell) = self.lstm(embedded)\n",
        "        # hidden has shape: [num_layers, batch_size, hidden_dim]. Use the last layer.\n",
        "        hidden_last = hidden[-1]  # Shape: [batch_size, hidden_dim]\n",
        "        logits = self.fc(hidden_last)  # Shape: [batch_size, output_dim]\n",
        "        # Apply sigmoid for binary classification\n",
        "        return self.sigmoid(logits).squeeze()\n",
        "\n",
        "# Hyperparameters\n",
        "vocab_size = len(vocab)\n",
        "embed_dim = 100\n",
        "hidden_dim = 128\n",
        "output_dim = 1  # Binary classification\n",
        "\n",
        "model = RNNClassifier(vocab_size, embed_dim, hidden_dim, output_dim)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "NqF7H-YBXmky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the RNN model"
      ],
      "metadata": {
        "id": "6Pa7RzlzXxSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set training hyperparameters\n",
        "num_epochs = 3  # For demonstration, use a small number of epochs.\n",
        "learning_rate = 0.001\n",
        "\n",
        "criterion = nn.BCELoss()  # Binary cross-entropy loss for binary classification.\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Move model to appropriate device (CPU for this demo).\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "criterion.to(device)\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    for texts, labels in train_loader:\n",
        "        texts, labels = texts.to(device), labels.float().to(device)\n",
        "        optimizer.zero_grad()         # Reset gradients.\n",
        "        predictions = model(texts)      # Forward pass.\n",
        "        loss = criterion(predictions, labels)  # Compute prediction error.\n",
        "        loss.backward()               # Backpropagate error.\n",
        "        optimizer.step()              # Update weights.\n",
        "        epoch_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader):.4f}\")"
      ],
      "metadata": {
        "id": "wVqVnhCeXzAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the Model"
      ],
      "metadata": {
        "id": "NQ_aWujwX2Nx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict sentiment on a single review text.\n",
        "def predict_sentiment(model, text):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Convert text to tensor and add batch dimension.\n",
        "        text_tensor = torch.tensor(text_pipeline(text)).unsqueeze(0).to(device)\n",
        "        prediction = model(text_tensor)\n",
        "        # Return sentiment label based on threshold 0.5.\n",
        "        sentiment = \"Positive\" if prediction.item() >= 0.5 else \"Negative\"\n",
        "        return sentiment, prediction.item()\n",
        "\n",
        "# Try predicting sentiment on a few sample reviews.\n",
        "sample_reviews = [\n",
        "    \"I absolutely loved this movie. It was amazing and full of surprises!\",\n",
        "    \"The film was boring and too long. I wouldn't recommend it.\",\n",
        "    \"Not the best movie I've seen, but it had its moments.\",\n",
        "    \"This movie was so bad it made me laugh at how terrible it was!\"\n",
        "]\n",
        "\n",
        "for review in sample_reviews:\n",
        "    sentiment, score = predict_sentiment(model, review)\n",
        "    print(f\"Review: {review}\\nPredicted Sentiment: {sentiment} (Score: {score:.4f})\\n\")\n"
      ],
      "metadata": {
        "id": "8xRe1PrkX6V6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wrap-Up\n",
        "\n",
        "In this notebook, we:\n",
        "- Loaded and preprocessed the IMDB dataset.\n",
        "- Built a vocabulary and converted text to numerical data.\n",
        "- Defined a custom RNN (LSTM-based) for sentiment classification.\n",
        "- Trained the model using a simple training loop.\n",
        "- Made predictions on sample reviews for fun sentiment analysis."
      ],
      "metadata": {
        "id": "xVqNQFwVYMTB"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}