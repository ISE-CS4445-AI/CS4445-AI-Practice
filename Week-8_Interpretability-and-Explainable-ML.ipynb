{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ae3d080cbbcf3ca8",
      "metadata": {
        "id": "ae3d080cbbcf3ca8"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ISE-CS4445-AI/CS4445-AI-Practice/blob/main/Week-8_Interpretabilit-and-Explainable-ML.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f517864226745601",
      "metadata": {
        "id": "f517864226745601"
      },
      "source": [
        "# **Week 8 Exercise: Interpretability and Explainable ML**\n",
        "\n",
        "Welcome to **Week 8**!\n",
        "\n",
        "This notebook explores techniques for interpreting and explaining machine learning models, focusing on object detection.\n",
        "\n",
        "We'll use a pre-trained object detector and visualize layer activations to understand how the model identifies objects.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9099c96f9ad1ad63",
      "metadata": {
        "id": "9099c96f9ad1ad63"
      },
      "source": [
        "## 1. Imports and setup\n",
        "Include all the required imports and setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aMxvyOxgh1s4",
      "metadata": {
        "id": "aMxvyOxgh1s4"
      },
      "outputs": [],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3f0940dbd5eb623",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-05T01:10:01.818722Z",
          "start_time": "2025-03-05T01:10:01.816466Z"
        },
        "id": "c3f0940dbd5eb623"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
        "from torchinfo import summary\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import PIL.Image as Image\n",
        "\n",
        "# Check for CUDA availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70398665e6bf8344",
      "metadata": {
        "id": "70398665e6bf8344"
      },
      "source": [
        "## 2. Load Pre-trained Model and Example Image\n",
        "\n",
        "Now we load a pre-trained Faster R-CNN model from Torchvision. This model is designed for object detection tasks, and we set it to evaluation mode since we are not training it. We also define image transformations to convert input images into the format expected by the model. This cell lays the groundwork for passing an image through the model to later visualize hidden activations and compute interpretability measures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "621a1e10d2657b6b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-05T01:12:47.807651Z",
          "start_time": "2025-03-05T01:12:47.550262Z"
        },
        "id": "621a1e10d2657b6b"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained Faster R-CNN model\n",
        "model = fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT).to(device)\n",
        "model.eval()\n",
        "\n",
        "# Define image transformations\n",
        "transform = transforms.Compose([\n",
        "    # transforms.Resize((800, 800)),  # Resize to a larger fixed shape for detection\n",
        "    transforms.ToTensor()\n",
        "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a164a15b43fce777",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-05T01:55:17.185099Z",
          "start_time": "2025-03-05T01:55:17.182862Z"
        },
        "id": "a164a15b43fce777"
      },
      "outputs": [],
      "source": [
        "# Summary of the model to look at the layers\n",
        "summary(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "picg0iLDi_4D",
      "metadata": {
        "id": "picg0iLDi_4D"
      },
      "outputs": [],
      "source": [
        "print(\"\\nDetailed Layer Information:\")\n",
        "for name, layer in model.named_modules():  # Iterate through named modules\n",
        "    print(f\"Layer Name: {name}\")\n",
        "    # Add any layer-specific information you want to print here\n",
        "    # Example: Print layer type\n",
        "    print(f\"Layer Type: {type(layer).__name__}\")\n",
        "    print(\"-\" * 20)  # Separator"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "254ce0ec7f7cabdf",
      "metadata": {
        "id": "254ce0ec7f7cabdf"
      },
      "source": [
        "## 3. Visualize Layer Activations\n",
        "\n",
        "Layer activation visualization is a powerful tool to understand what features a model extracts at various depths. In this section, we define a function that registers a forward hook on a specified layer of the model. As the image passes through the network, the hook collects the activation maps produced by that layer. These activation maps are then aggregated, normalized, and overlaid as a heatmap on the original image — providing visual insights into which parts of the image are emphasized by the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33d226bffc1da9b9",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-05T01:12:16.533Z",
          "start_time": "2025-03-05T01:12:16.530685Z"
        },
        "id": "33d226bffc1da9b9"
      },
      "outputs": [],
      "source": [
        "# COCO class names\n",
        "COCO_CLASSES = [\n",
        "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
        "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
        "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
        "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
        "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
        "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
        "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
        "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
        "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
        "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
        "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
        "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d869b62048e620d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-05T01:15:45.601444Z",
          "start_time": "2025-03-05T01:15:45.591849Z"
        },
        "id": "7d869b62048e620d"
      },
      "outputs": [],
      "source": [
        "def visualize_layer_activation(model, image, layer_name):\n",
        "    \"\"\"\n",
        "    Visualize the activations of a specific layer in the model\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The neural network model\n",
        "        image (torch.Tensor): Input image tensor [1, C, H, W]\n",
        "        layer_name (str): Name of the layer to visualize\n",
        "\n",
        "    Returns:\n",
        "        dict: Detection results from the model\n",
        "    \"\"\"\n",
        "    activation = None  # Initialize activation as None\n",
        "\n",
        "    def hook(module, input, output):\n",
        "        nonlocal activation  # Modify the outer scope activation variable\n",
        "        activation = output\n",
        "\n",
        "    # Find the target layer by name using a recursive helper function\n",
        "    def find_layer_by_name(module, layer_name):\n",
        "        for name, layer in module.named_modules():\n",
        "            if name == layer_name:\n",
        "                return layer\n",
        "        return None\n",
        "\n",
        "    target_layer = find_layer_by_name(model, layer_name)\n",
        "\n",
        "    if target_layer is None:\n",
        "        print(f\"Layer '{layer_name}' not found in the model.\")\n",
        "        return\n",
        "\n",
        "    hook_handle = target_layer.register_forward_hook(hook)\n",
        "\n",
        "    result = model(image)  # Forward pass to activate the hook and retrieve activation\n",
        "\n",
        "    hook_handle.remove()  # Remove the hook\n",
        "\n",
        "    # Use a dummy activation map if activation is still None (layer not returning output)\n",
        "    if activation is None:\n",
        "        print(f\"WARNING: Layer '{layer_name}' returned None. Visualizing dummy activation map.\")\n",
        "        activation = torch.zeros((1, 1, image.shape[2], image.shape[3]))\n",
        "\n",
        "    # Process activation and generate heatmap\n",
        "    activation = activation.mean(dim=1).squeeze()\n",
        "    activation = (activation - activation.min()) / (activation.max() - activation.min())\n",
        "    activation = activation.cpu().detach().numpy()\n",
        "\n",
        "    image_np_original = (image.squeeze().permute(1, 2, 0).cpu().detach().numpy() * 255).astype(np.uint8)\n",
        "\n",
        "    # Resize and overlay heatmap on the original image\n",
        "    heatmap = cv2.resize(activation, (image.shape[3], image.shape[2]))\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
        "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)  # Convert to RGB for matplotlib\n",
        "    image_np = image.squeeze().permute(1, 2, 0).cpu().detach().numpy()  # Convert image to NumPy\n",
        "    overlayed_image = heatmap * 0.5 + (image_np * 255) * 0.5\n",
        "\n",
        "    labels = result[0]['labels'].cpu().numpy()\n",
        "    boxes = result[0]['boxes'].cpu().detach().numpy()\n",
        "    scores = result[0]['scores'].cpu().detach().numpy()\n",
        "\n",
        "    image_with_boxes = image_np_original.copy()\n",
        "\n",
        "    if len(labels) > 0 and len(boxes) > 0:\n",
        "        # Only process the first result\n",
        "        image_with_boxes = image_np_original.copy()\n",
        "        for i in range(min(len(boxes), 5)):\n",
        "            box = boxes[i]\n",
        "            label = labels[i]\n",
        "            score = scores[i]\n",
        "\n",
        "            x1, y1, x2, y2 = map(int, box)\n",
        "            class_name = COCO_CLASSES[label]  # Map label to class name\n",
        "            cv2.rectangle(image_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 3)  # Green box\n",
        "            cv2.putText(\n",
        "                image_with_boxes,\n",
        "                f'Pred {i+1}: {class_name}, Confidence: {score * 100:.2f}%',  # Overlay the class name\n",
        "                (x1, y1 - 5),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                0.5,\n",
        "                (255, 0, 0),  # Blue text\n",
        "                2\n",
        "            )\n",
        "\n",
        "    # Add progress feedback\n",
        "    print(f\"Processing layer: {layer_name}\")\n",
        "    print(f\"Activation shape: {activation.shape}\")\n",
        "\n",
        "    # Create separate figures for activation and detection\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # plt.subplot(1, 3, 1)\n",
        "    # plt.imshow(image_np_original)\n",
        "    # plt.title(\"Original Image\")\n",
        "    # plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(image_with_boxes.astype(np.uint8))\n",
        "    plt.title(\"Detection Results\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(overlayed_image.astype(np.uint8))\n",
        "    plt.title(f\"Layer Activation: {layer_name}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return [COCO_CLASSES[label] for label in labels[:5]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2cca023",
      "metadata": {},
      "source": [
        "### How the above function works\n",
        "1. This function begins by defining a hook that captures the output (`activation`) from a specified layer during the forward pass. \n",
        "2. We search for the layer within the model using `named_modules()`, and if the layer is found, we register the forward hook. \n",
        "3. The image is then passed through the model (in evaluation mode and without gradient tracking), which triggers the hook that saves the activation map.\n",
        "4. After the forward pass, we remove the hook to avoid unintended side effects. \n",
        "5. The function then computes a mean across channels, normalizes the activations, and resizes the activation map to match the original image dimensions. \n",
        "6. The activation map is converted to a heatmap using OpenCV, then blended with the original image. \n",
        "7. The prediction is also visualised by drawing the bounding boxes over the original image using opencv.\n",
        "8. Finally, the result is displayed using Matplotlib. This provides insights into which regions of the image strongly activate the specified layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ShZ6V6BmPt12",
      "metadata": {
        "id": "ShZ6V6BmPt12"
      },
      "outputs": [],
      "source": [
        "def loadImage(imagePath):\n",
        "  # Load an example image\n",
        "  image = Image.open(imagePath)  # Replace with your image path\n",
        "  image = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "  # Visualize the preprocessed image\n",
        "  plt.figure(figsize=(8, 8))\n",
        "  grid = make_grid(image.cpu(), nrow=1)  # Remove batch dimension for display\n",
        "  plt.imshow(grid.permute(1, 2, 0))  # Convert CHW format to HWC for Matplotlib\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "  return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q_mFW9FeMjvn",
      "metadata": {
        "id": "q_mFW9FeMjvn"
      },
      "outputs": [],
      "source": [
        "!mkdir -p datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lQOmQ5rxNRPC",
      "metadata": {
        "id": "lQOmQ5rxNRPC"
      },
      "outputs": [],
      "source": [
        "!wget -p https://raw.githubusercontent.com/sprince0031/CS4445-AI-Practice/refs/heads/staging/datasets/dog.jpg -O datasets/dog.jpg\n",
        "!wget -p https://raw.githubusercontent.com/sprince0031/CS4445-AI-Practice/refs/heads/staging/datasets/cat-and-dog.jpeg -O datasets/cat-and-dog.jpg\n",
        "!wget -p https://raw.githubusercontent.com/sprince0031/CS4445-AI-Practice/refs/heads/staging/datasets/tiger.jpg -O datasets/tiger.jpg\n",
        "!wget -p https://raw.githubusercontent.com/sprince0031/CS4445-AI-Practice/refs/heads/staging/datasets/dinning-table.jpg -O datasets/dining-table.jpg\n",
        "!wget -p https://raw.githubusercontent.com/sprince0031/CS4445-AI-Practice/refs/heads/staging/datasets/tintin-meme.jpg -O datasets/tintin-meme.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97879a7fb697c141",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-05T00:51:37.551515Z",
          "start_time": "2025-03-05T00:51:37.267502Z"
        },
        "id": "97879a7fb697c141"
      },
      "outputs": [],
      "source": [
        "# Load an example image\n",
        "image = loadImage(\"datasets/dog.jpg\")\n",
        "\n",
        "preds = visualize_layer_activation(model, image, \"backbone.body.layer4\")\n",
        "print(preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1aa7bc690feb30dd",
      "metadata": {
        "id": "1aa7bc690feb30dd"
      },
      "source": [
        "## 4. Exercise: Interpret Activations\n",
        "\n",
        "Analyze the generated visualizations to understand what parts of the image the model focuses on when detecting objects. Reflect on these points:\n",
        "* Do the highlighted regions correspond to the objects themselves or to background features?\n",
        "* How might these activations relate to the model’s confidence in its detections?\n",
        "*  Try visualizing different layers to see how representations evolve through the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73132a9548db68d6",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-05T00:51:42.824780Z",
          "start_time": "2025-03-05T00:51:42.741164Z"
        },
        "id": "73132a9548db68d6"
      },
      "outputs": [],
      "source": [
        "# Load an example image\n",
        "image = loadImage(\"datasets/cat-and-dog.jpg\")\n",
        "\n",
        "preds = visualize_layer_activation(model, image, \"backbone.body.layer4\")\n",
        "print(preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdc8751afdcf7909",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-05T00:51:58.033375Z",
          "start_time": "2025-03-05T00:51:57.450775Z"
        },
        "id": "bdc8751afdcf7909"
      },
      "outputs": [],
      "source": [
        "image = loadImage(\"datasets/tiger.jpg\")\n",
        "\n",
        "preds = visualize_layer_activation(model, image, \"backbone.body.layer4\")\n",
        "print(preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "438c8995e9f41d37",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-05T02:01:49.924750Z",
          "start_time": "2025-03-05T02:01:49.727448Z"
        },
        "id": "438c8995e9f41d37"
      },
      "outputs": [],
      "source": [
        "# Load an example image\n",
        "image = loadImage(\"datasets/dining-table.jpg\")  # Replace with your image path\n",
        "preds = visualize_layer_activation(model, image, \"backbone.body.layer4\")\n",
        "print(preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee971cb96564d2a7",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-05T00:51:50.767009Z",
          "start_time": "2025-03-05T00:51:50.595265Z"
        },
        "id": "ee971cb96564d2a7"
      },
      "outputs": [],
      "source": [
        "# Load an example image\n",
        "image = loadImage(\"datasets/tintin-meme.jpg\")  # Replace with your image path\n",
        "preds = visualize_layer_activation(model, image, \"backbone.body.layer4\")\n",
        "print(preds)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
