{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Week 6 Exercise: Noise Handling & Regularization**\n",
    "\n",
    "Welcome to **Week 6**! We’re focusing on **noise handling** and **regularization** techniques. This exercise demonstrates:\n",
    "\n",
    "1. **Data Augmentation** (random crops, flips) to handle noise / robustify training.  \n",
    "2. **Dropout** in a small CNN architecture.  \n",
    "3. **Weight Decay** (L2 regularization) in the optimiser.\n",
    "\n",
    "We’ll use **CIFAR-10** (10 classes: airplane, car, bird, cat, etc.), a moderate-sized dataset of 32×32 color images. By applying these techniques, we aim to reduce overfitting and improve robust performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions\n",
    "\n",
    "Defining the utility function to plot metrics below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Description:\n",
    "Utility function to plot line plots in a single graph\n",
    "\n",
    "Params:\n",
    "dataDict: List - A list of tuples containing the data to be plotted and corresponding plot params as a dictionary (optional).\n",
    "           Expected key names in paramDict:\n",
    "           -> label: str - String label to be given to the plot. \n",
    "                           Only necessary if the 'legend' function param is set to True (False by default).\n",
    "           -> ha: str - Specifies the horizontal alignment ('left', 'right' or 'center') of text above each point in the plot.\n",
    "           -> fontsize: int - Sets the font size of text displayed above each point in the plot.\n",
    "           -> marker: str - Sets the style of marker to be displayed for each data point on the plot. Set to 'o' by default.\n",
    "           -> decimalPlaces: int - Sets the number of decimal places to display for each data point.\n",
    "           -> displayPercent: bool - Boolean to decide whether to display numbers in percentage format.\n",
    "           -> displayOffset: float - positive or negative float value that determines the display offset of text above data point.\n",
    "title: str - [optional] Title to be set for the graph.\n",
    "xlabel: str - [optional] Label for the x-axis to be set for the graph.\n",
    "ylabel: str - [optional] Label for the y-axis to be set for the graph.\n",
    "figSize: Tuple - [optional] Sets a custom figure size for the plot based on the width and height values passed as a tuple pair.\n",
    "legend: bool - [optional] Boolean to decide whether to show the legend or not. Set to False by default\n",
    "'''\n",
    "def plotMetrics(dataList, X, title='', xlabel='', ylabel='', figSize=None, legend=False):\n",
    "    if figSize:\n",
    "            plt.figure(figsize=(figSize))\n",
    "    for data in dataList:\n",
    "        y, paramDict = data\n",
    "        # Getting plot params\n",
    "        label = paramDict['label'] if 'label' in paramDict else ''\n",
    "        marker = paramDict['marker'] if 'marker' in paramDict else 'o'\n",
    "        ha = paramDict['ha'] if 'ha' in paramDict else 'center'\n",
    "        fontSize = paramDict['fontSize'] if 'fontSize' in paramDict else 8\n",
    "        decimalPlaces = paramDict['decimalPlaces'] if 'decimalPlaces' in paramDict else 2\n",
    "        displayPercent = paramDict['displayPercent'] if 'displayPercent' in paramDict else False\n",
    "        displayOffset = paramDict['displayOffset'] if 'displayOffset' in paramDict else 0.005\n",
    "        \n",
    "        plt.plot(X, y, label=label, marker=marker)\n",
    "        \n",
    "        # Getting the data values to show on the plotted points along the line\n",
    "        for i, v in enumerate(y):\n",
    "            percentMultiplier = 100 if displayPercent else 1\n",
    "            v_str = f'{v * percentMultiplier:.{decimalPlaces}f}{\"%\" if displayPercent else \"\"}'\n",
    "            plt.text(i + 1, v + displayOffset, v_str, ha=ha, fontsize=fontSize)\n",
    "        \n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    if legend:\n",
    "        plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Load & Augment CIFAR-10\n",
    "\n",
    "**Task**:  \n",
    "1. Use **transforms** for data augmentation: random crop, horizontal flip.  \n",
    "2. Convert to `Tensor`, normalize.  \n",
    "3. Create `DataLoader` for train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTransforms(augmentation=False):\n",
    "    # mean,std for CIFAR10\n",
    "    cifar_mean = (0.4914, 0.4822, 0.4465)\n",
    "    cifar_std  = (0.2470, 0.2435, 0.2616)\n",
    "    \n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(cifar_mean, cifar_std)\n",
    "    ])\n",
    "\n",
    "    if augmentation:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),  # basic data aug\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(cifar_mean, cifar_std)\n",
    "        ])\n",
    "        return train_transform, test_transform    \n",
    "\n",
    "    return test_transform, test_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10_data(batch_size=64, augmentation=False):\n",
    "    \"\"\"\n",
    "    1. define train_transform with random crop, flip, normalization\n",
    "    2. define test_transform with just resize or basic normalization\n",
    "    3. load CIFAR10 train/test\n",
    "    4. create DataLoader for each\n",
    "    return train_loader, test_loader\n",
    "    \"\"\"\n",
    "    \n",
    "    train_transform, test_transform = getTransforms(augmentation)\n",
    "\n",
    "    # TODO: set download=True if first time\n",
    "    train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=train_transform)\n",
    "    test_dataset  = torchvision.datasets.CIFAR10(root='./data', train=False, download=False, transform=test_transform)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "train_loader, test_loader = load_cifar10_data(batch_size=64)\n",
    "print(\"Train set size:\", len(train_loader.dataset))\n",
    "print(\"Test set size:\", len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define a CNN with Dropout\n",
    "\n",
    "**Task**:  \n",
    "- Build a small **CNN** with a few convolution layers, each followed by ReLU & maybe pool.  \n",
    "- Insert **dropout** layers (e.g., `nn.Dropout(0.3)`) to help reduce overfitting.  \n",
    "- The final linear outputs 10 classes.  \n",
    "- We'll call it `NetCIFAR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetCIFAR(nn.Module):\n",
    "    def __init__(self, dropout_prob=0.3, useRegularization=False):\n",
    "        super().__init__()\n",
    "        self.useRegularization = useRegularization\n",
    "        # example architecture\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool  = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        if self.useRegularization:\n",
    "            self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128*8*8, 256)  # after 2 pools => 8x8 size\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # shape x: (batch,3,32,32)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)  # (batch,64,16,16)\n",
    "        \n",
    "        if self.useRegularization:\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool(x)  # (batch,128,8,8)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # flatten => (batch, 128*8*8)\n",
    "        if self.useRegularization:\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        if self.useRegularization:\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        out = self.fc2(x)\n",
    "        # no softmax, we use CrossEntropyLoss\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Loop with Weight Decay\n",
    "\n",
    "**Task**:  \n",
    "- We define `train_model(...)` that uses:\n",
    "  - Adam or SGD with **weight_decay** param (like 1e-4).  \n",
    "  - `nn.CrossEntropyLoss()`.  \n",
    "  - Standard PyTorch training loop (epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimiser):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # This is the backprop set up. Explain what each of the steps do\n",
    "        loss.backward()        \n",
    "        optimiser.step()       \n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    loss, accuracy = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss += loss_fn(pred, y).item()\n",
    "            accuracy += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    loss /= num_batches\n",
    "    accuracy /= size\n",
    "\n",
    "    return loss, accuracy\n",
    "\n",
    "def train_loop(train_dataloader, test_dataloader, model, loss_fn, optimiser, epochs):\n",
    "    train_loss = []\n",
    "    train_accuracy = []\n",
    "    test_loss = []\n",
    "    test_accuracy = []\n",
    "\n",
    "    # Iterate over each epoch\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}:\\n\")\n",
    "        train(train_dataloader, model, loss_fn, optimiser)\n",
    "\n",
    "        # Get the overall loss and accuracy for both train and test datasets\n",
    "        tr_loss, tr_acc = test(train_dataloader, model, loss_fn)\n",
    "        ts_loss, ts_acc = test(test_dataloader, model, loss_fn)\n",
    "\n",
    "        print(f\"Train Error: \\n Accuracy: {(100*tr_acc):>0.1f}%, Avg loss: {tr_loss:>8f} \\n\")\n",
    "        print(f\"Test Error: \\n Accuracy: {(100*ts_acc):>0.1f}%, Avg loss: {ts_loss:>8f} \\n\")\n",
    "\n",
    "        # Store and return the losses and accuracies. We can graph these later\n",
    "        train_loss = train_loss + [tr_loss]\n",
    "        train_accuracy = train_accuracy + [tr_acc]\n",
    "        test_loss = test_loss + [ts_loss]\n",
    "        test_accuracy = test_accuracy + [ts_acc]\n",
    "\n",
    "    print(\"Done training!\")\n",
    "    return train_loss, train_accuracy, test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate & Check Test Accuracy\n",
    "\n",
    "**Task**:  \n",
    "1. Measure train and test metrics for non-regularised model with non-augmented data.  \n",
    "2. Do the same train and test loop for your regularised model on augmented data with weight decay in the optimiser and observe the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-regularised model with non-augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NetCIFAR() # Getting the LeNet5 model\n",
    "\n",
    "# Define the loss function and the optimiser\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "train_loss, train_accuracy, test_loss, test_accuracy = train_loop(train_loader, test_loader, model, loss_fn, optimiser, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochRange = range(1, epochs+1)\n",
    "# Defining data and plot params\n",
    "lossDataList = [(train_loss, {'label': 'Train loss', 'decimalPlaces': 4, 'displayOffset': -0.009}), \n",
    "                (test_loss, {'label': 'Test loss', 'decimalPlaces': 4, 'displayOffset': -0.012})]\n",
    "plotTitle = 'Train and Test Loss for LeNet-5 model'\n",
    "\n",
    "# Calling my custom util function to plot loss data\n",
    "plotMetrics(lossDataList, epochRange, xlabel='Epochs', ylabel='Loss', title=plotTitle, legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining data and plot params\n",
    "accuracyDataList = [(train_accuracy, {'label': 'Train accuracy', 'displayOffset': 0.0015,\n",
    "                                      'decimalPlaces': 2, 'displayPercent': True}), \n",
    "                (test_accuracy, {'label': 'Test accuracy', 'ha': 'left', 'displayOffset': -0.003,\n",
    "                                 'decimalPlaces': 2, 'displayPercent': True})]\n",
    "plotTitle = 'Train and Test Accuracy for LeNet-5 model'\n",
    "\n",
    "# Calling my custom util function to plot accuracy data\n",
    "plotMetrics(accuracyDataList, epochRange, xlabel='Epochs', ylabel='Accuracy', title=plotTitle, legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With regularisation and data augmentation applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = load_cifar10_data(batch_size=64, augmentation=True)\n",
    "\n",
    "model = NetCIFAR(useRegularization=True)\n",
    "\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4) # L2 regularization with weight_decay\n",
    "train_loss, train_accuracy, test_loss, test_accuracy = train_loop(train_loader, test_loader, model, loss_fn, optimiser, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining data and plot params\n",
    "lossDataList = [(train_loss, {'label': 'Train loss', 'decimalPlaces': 4, 'displayOffset': -0.009}), \n",
    "                (test_loss, {'label': 'Test loss', 'decimalPlaces': 4, 'displayOffset': -0.012})]\n",
    "plotTitle = 'Train and Test Loss for LeNet-5 model'\n",
    "\n",
    "# Calling my custom util function to plot loss data\n",
    "plotMetrics(lossDataList, epochRange, xlabel='Epochs', ylabel='Loss', title=plotTitle, legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining data and plot params\n",
    "accuracyDataList = [(train_accuracy, {'label': 'Train accuracy', 'displayOffset': 0.0015,\n",
    "                                      'decimalPlaces': 2, 'displayPercent': True}), \n",
    "                (test_accuracy, {'label': 'Test accuracy', 'ha': 'left', 'displayOffset': -0.003,\n",
    "                                 'decimalPlaces': 2, 'displayPercent': True})]\n",
    "plotTitle = 'Train and Test Accuracy for LeNet-5 model'\n",
    "\n",
    "# Calling my custom util function to plot accuracy data\n",
    "plotMetrics(accuracyDataList, epochRange, xlabel='Epochs', ylabel='Accuracy', title=plotTitle, legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "**Week 6**: We applied **data augmentation** (random crop, flip) to handle noise or robustify input data, used **Dropout** in the CNN, and included **Weight Decay** in the optimiser. These regularization techniques help reduce overfitting and handle possible noise, aiming for more stable performance.\n",
    "\n",
    "**End of Week 6 Exercise** – Tweak hyperparams, see if you can push test accuracy further while avoiding overfitting."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
